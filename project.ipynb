{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science — Final Project\n",
    "*MATH 4100 / COMP 5360, University of Utah, http://datasciencecourse.net/*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Information\n",
    "\n",
    "Please fill in the following details:\n",
    "\n",
    "- **Project Name:**  Guiding Optimizations with Gershgorin Discs    \n",
    "- **Team Names:**  Joel Wood || Sasha Kushnir  \n",
    "- **Emails:**  joel.wood@ngc.com ||     \n",
    "- **UIDs:**  u0499977 ||  u1497176"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Motivation\n",
    "\n",
    "The fields of Operations Research and Convex Optimization form a cornerstone of applied mathematics, in which optimal combinations of variables and constraints are solved in order to max/min an objective function.   This form of optimization is used throughout many industries to help solve np-hard problems, by determining the best decisions in how to allocate resources which are governed by many complex constraints, such as:\n",
    "- **Logistics:** - Solving \"Traveling Salesman\", \"Knapsack\", and \"Vehicle Routing\" problems to minimize shipping times, minimize number of trips, and reducing fuel usage.  Linear Programming was developed by George Danzig during WW2 to opimize the logistics of the US military, enabling the US to efficiently supply their operations with an unprecedented efficiency--the Simplex LP algorithm is considered one of the greatest innovations in applied mathematics.  Danzig is actually the inspiration of the movie \"Good Will Hunting\", in which, after arriving to class late, he \"accidentally\" solved two of the most famous unsolved statistics problems, which Danzig had mistakenly assumed were unusually difficult homework problems.\n",
    "- **Finance:** - Optimizing portfolios with the goal to maximize returns for a specified level of risk (\"efficient frontier\").  It helps guide, based on risks and probabilities, how to optimally invest assets and develop stable cash-flows.\n",
    "- **Manufacturing:** - Determining the optimal way to allocate resources across different product lines and determining optimal build sequences and batch sizing to maximize profits meeting customer demands while mitigating excess Inventory (\"JIT Manufacturing\").\n",
    "- **Scheduling:** - Determining optimal schedules which consider both customer and employee needs, ensuring the right people are assigned where they are needed, while making sure schedules are compliant with labor laws.  OR assisted scheduling of flights and their crews substantially reduced flight costs and govern stochastic pricing strategies, slashing the cost of airfare for consumers and saving airlines tens of millions of dollars each year. \n",
    "- **Power Grids:** - Power grids have to consider the balance of electricity supply and demand across whole continents, in which fluctuations due to weather and other events require a balance between different power generation strategies (back-up generators) and building robust infrastructure.  This involves a balance of risk and costs, with catastrophic results from not properly balancing these many uncertain factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These problems explode exponentially, often exceeding billions of different combinations.  With binary variables themselves contributing to a combinatorial explosion, forming MILP (Mixed-Integer Linear Programs) which have more branch-and-bound tree leaves than there are atoms in the universe.  The memory and time requirements to solve many such problems become \"impossible\" to achieve with a generalized algorithm.  There is no \"best\" algorithm, and thus an open problem in these fields is better understanding how an optimal algorithm can be applied generally--similar to an agentic assignment of strategies.  Some common (but not inclusive) issues which require tuning to solve OR problems include:\n",
    "- **Pivoting:** - Using Simplex LP, decision pivots move through the outer hull of the multi-dimensional space, often utilizing a \"warm-start\" in which a known viable configuration is used within a sparse system.   However, there are also Interior Point methods which seek to reduce iterations by traversing not the edge, but the interior of this hull.  These interior methods increase speed while being prone to matrix degeneracy, getting stuck at local optima, or becoming more resource intensive.\n",
    "- **Pre-Solves:** - Many problems are initialized with a heuristic which will find an intersection of constraints and variables to eliminate redundant combinations (reducing the matrix size) and further reducing the problem space's feasible region.  This is tied to the \"warm-start\" in which you choice of a starting point will greatly impact on the number of iterations required, and also influence the ideal pivot techniques to traverse your redefined feasible region.\n",
    "- **Numerical Stability:** - due to repeatedly needing to solve massive matrices, if the matrix is \"ill-conditioned\" where minor changes cause large fluctuations, the cost of individual calculations may become more problematic than simply doing additional stable calculations.  In these situations, controlling your matrix sparsity and numerical precision/stability becomes more important than reducing computations.\n",
    "- **Slack and Hard-Constraints:** - Dual Simplex uses slack variables to better balance the variables to constraints, allowing for matrices which can \"fail\" to meet soft constraints with high penalties.  However, these slack variable penalties must be set high enough to enforce them, without causing numerical instability.  Sometimes setting a massive value will allow for fast solves, while sometimes it will cause abrupt crashes.  Similarly, within MILP problems, the use of constraints to shrink the feasible space may allow you to prune your branch and bound tree more effectively, but it may also make it more difficult to even find \"feasible\", much less optimal.  This creates a problem in which balance and structure of a matrix may make one combination solve in minutes, while another may take years (i.e. functionally impossible).   This makes generalized solvers difficult to make for even a particular type of problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to a lack of a generalized best algorithm and parameter balance for these problems, finding computationally efficient ways to understand underlying matrix structure and clustering variable types into \"topologically\" distinct groups allows for more guided and generalized ways to perform pre-solving for an ideal starting point within the feasible region, and also to improve pivot selections based on principles of \"spectral analysis\" to allow parallel or possibly convolutional computations across larger regions of the convex hull, as well as an understanding of degeneracy and sparsity fill-in risks from doing so.   It is believed that a stronger understanding of how to efficiently predict both structure and variable clustering within matrices will empower a more generalized approach to Convex Optimization, in which a more specialized approach can be assigned based on a computationally cheap preassessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the methods which may be powerful in the context of Operations Research in which there are massive, sparse, and positive definite matrices, is the use of Gershgorin Discs.   The calculation of Gershgorin discs forms circles within the complex plane, the union of these circles represent the region in which those variable's eigenvalues exist.  This is functionally useless in dense matrices or ill-conditioned matrices, as everything becomes a single massive union.  But properly balanced and sparse matrices will likely form distinct clusters of eigenvalues.  This method fails to find specific eigenvalues and does not reveal their vectors.  However, as seen below, this more generalized information can be derived at a much lower cost than computing out all the eigen pairs.\n",
    "- **Gershgorin Discs:** - O(m+n) where m represents non-zero entries, in the sparse matrices found in most OR problems, this is functionally Amortized O(n), a linear algorithm. \n",
    "- **Gershgorin Bipartite Laplacian:** - due to OR usually having imbalances in the number of rows and columns (variables and constraints), you will need to form a new Bipartite Laplacian to be able to find eigenvalues for the system.  The formation of the Bipartite Laplacian is functionally O(m) within sparse matrices.  Because a Laplacian is diagonally dominant the calculation of Gershgorin Discs is further simplified.  This leads to an overall complexity of both algorithms to be approximately O(nm), more time consuming than finding the discs, but this will often be around Amortized O(n) or O(logn) within these sparse matrices.\n",
    "- **Scaled Gershgorin Discs:** - due to the density of the RHS in these problems, if Gershgorin discs are not forming distinct unions, it is possible to use a Linear Program to find a diagonal scaling matrix which will adjust their radii or align them to their RHS values.  These scaling problems are generally much simpler to solve than the typical linear program, and within sparse matrices, they have a complexity of O(km) where k is the number of iterations required by the sub LP.  This number is often significantly less than m, often leading to a roughly linear complexity in sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, all of these algorithms have complexities which are significantly less than computing eigenpairs directly, which is O(n^3)—absolutely massive in a matrix with millions of rows and columns.  There are ways to compute a subset of needed eigenpairs within large sparse matrices, but they are similarly not reliable in their complexity, based on the very factors which influence Linear Programs.  Which therefore, the Gershgorin Disc unions, can lead to more efficient and targetted Eigenpair extraction if needed for a later optimization strategy.  It is therefore believed that this low cost, which is often around O(n), can be used to guide a generalized structure of predictive “optimal” optimization strategies within the fields of Operations Research and Convex Optimization.  If a probabilistic strategy can be developed to use Gershgorin Discs to make Linear Programs more generalized to solve problems more consistently, it has the potential to drive agentic improvements across many high-impact industries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Objectives\n",
    "\n",
    "Our group members have an academic, professional, and personal interest in the intersection of statistics, operations research, differential equations and dynamic systems.  Our objective is to better understand matrix properties, structures, eigenvalues, and how these can be used to predict each other—with an overlap to random matrix theory.  The end goal of this project is to find new avenues of research for this team, to determine feasibility of existing research ideas, and to form a cornerstone of numerical analysis to justify and support further optimization research into the solving of industry relevant problems.\n",
    "\n",
    "Our primary question is therefore, \"Can you use Gershgorin Discs, to predict structure and properties of large, sparse, positive-definite matrices?  And can you similarly use any of these structures and properties to predict eigenvalue distributions within structured matrices?\"  The answer to this question may provide an opportunity to develop generalized algorithm selection techniques, which would drive innovation of algorithms which provide billions of dollars in efficiency savings throughout society.  The current global economy is built on the scalability which Linear Programming has supported, allowing for a level of efficiency within a global system which would not be possible through naive computations.  \n",
    "\n",
    "By unifying spectral/topological analysis with Linear Programming, it is believed that recent developments in Machine Learning can be unified with Linear Programming to improve both, and to further drive improved efficiencies within a plethora of fields.  This field of optimization is in high demand commerically, but academically it is often under-represented.  There are many open and unsolved problems in this domain, which represents a wealth of potential research topics and improvements beyond what is seen in many other fields.   We hope that the results of this project will give us insights and tools which can help us collectively find new optimization techniques--helping develop both applied mathematics and algorithmic improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "The "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethical Considerations\n",
    "\n",
    "The"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Processing\n",
    "\n",
    "How\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "\n",
    "Then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Methodology\n",
    "\n",
    "Also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Schedule\n",
    "\n",
    "And"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
